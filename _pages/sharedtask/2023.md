---
layout: single
title: "2023 BEA Shared Task"
subtitle: &subtitle "Generating AI Teacher Responses in Educational Dialogues" 
permalink: /sharedtask/2023
toc: true
sidebar:
  - title: *subtitle
    image: "/assets/images/sharedtask/2023.png"
    image_alt: "shared task illustration"
    text: "
Shared task organized during the [BEA](/bea/2023) workshop at [ACL 2023](https://2023.aclweb.org).<br>
Toronto, Canada <br>
July 13, 2023"
    nav: beasharedtask2023  # <-- TODO: add navigation for CodaLab links
---

<h1>{{ page.subtitle }}</h1>

## Motivation, Goal, and Purpose

Conversational agents offer promising opportunities for education. They can fulfill various roles (intelligent tutors and service-oriented assistants) and pursue different objectives (e.g., improving student skills and increasing instructional efficiency) {% cite wollny_are_2021 --file beasharedtask2023 %}. Among all of these different vocations of an educational chatbot, the most prevalent one is the **AI teacher** helping a student with skill improvement and providing more opportunities to practice. Some recent meta-analyses have even reported a significant effect of chatbots on skill improvement, for example in language learning {% cite bibauw_dialogue_2022 --file beasharedtask2023 %}. What is more, current advances in AI and natural language processing have led to the development of conversational agents that are founded on more powerful generative language models.

Despite these promising opportunities, the use of powerful generative models as a foundation for downstream tasks also presents several crucial challenges. In the educational domain in particular, it is important to ascertain whether that foundation is solid or flimsy. Bommasani et al. {% cite bommasani_opportunities_2021 --locator 67-72 --suppress_author --file beasharedtask2023 %} stressed that, if we want to put these models into practice as AI teachers, it is imperative to determine whether they can (a) speak to students like a teacher, (b) understand students, and (c) help students improve their understanding. Therefore, Tack and Piech {% cite tack_ai_2022 --suppress_author --file beasharedtask2023 %} formulated the **AI teacher test challenge**: How can we test whether state-of-the-art generative models are good AI teachers, capable of replying to a student in an educational dialogue?

Following the AI teacher test challenge, we organize a first **shared task on the generation of teacher language in educational dialogues**.
The goal of the task is to use NLP and AI methods to generate teacher responses in real-world samples of teacher-student interactions. These samples are taken from the *Teacher Student Chatroom Corpus* {% cite caines_teacherstudent_2020 caines_teacherstudent_2022 --file beasharedtask2023 %}. Each training sample is composed of a dialogue context (i.e., several teacher-student utterances) as well as the teacher’s response. For each test sample, participants are asked to submit their best generated teacher response.

    [ DIALOGUE CONTEXT ]
    Teacher: Yes, good! And to charge it up, you need to __ it ___
    Student: …
    Teacher: connect to the source of electricity 
    Student: i understand
    Teacher: plug it __?
    Student: in

    [ REFERENCE RESPONSE ]
    Teacher: yes, good. And when the battery is full, you need to ____ (disconnect it)

The purpose of the task is to **benchmark the ability of generative models to act as AI teachers, replying to a student in a teacher-student dialogue**. Submissions are ranked according to several automated dialogue evaluation metrics {% cite yeh_comprehensive_2021 --file beasharedtask2023 %}, with the top submissions selected for further human evaluation. During this manual evaluation, human raters compare a pair of teacher responses in terms of three abilities: speak like a teacher, understand a student, help a student {% cite tack_ai_2022 --file beasharedtask2023 %}. As such, we adopt an evaluation method that is akin to ACUTE-Eval for evaluating dialogue systems {% cite li_acuteeval_2019 --file beasharedtask2023 %}.

## Data

The shared task is based on data from the *Teacher-Student Chatroom Corpus* (TSCC) {% cite caines_teacherstudent_2020 caines_teacherstudent_2022 --file beasharedtask2023 %}. This corpus comprises data from several chatrooms (102 in total) in which an ESL teacher interacts with a student in order to work on a language exercise and assess the student’s English language proficiency. 

From each dialogue, several shorter passages were extracted. Each passage counts at most 100 tokens, is composed of several sequential teacher-student turns (i.e., the preceding dialogue context), and ends with a teacher utterance (i.e., the reference response). These short passages are the data samples used in this shared task.

The format is inspired by the JSON format used in ConvoKit {% cite chang_convokit_2020 --file beasharedtask2023 %}.
Each **training sample** is given as a JSON object composed of three fields:

id
: a unique identifier for this sample.

utterances
: a list of utterances, which corresponds to the preceding dialogue context. Each utterance is a JSON object with a ``"text"`` field containing the utterance and a ``"speaker"`` field containing a unique label for the speaker.

response
: a reference response, which corresponds to the final teacher’s utterance. Again, this utterance is a JSON object with a ``"text"`` field containing the utterance and a ``"speaker"`` field containing a unique label for the speaker.

~~~ json
{
    "id": "train_0000",
    "utterances": [
        {
            "text": "Yes, good! And to charge it up, you need to __ it ___",
            "speaker": "teacher",
        },
        {
            "text": "…",
            "speaker": "student",
        },
        {
            "text": "connect to the source of electricity",
            "speaker": "teacher",
        },
        {
            "text": "i understand",
            "speaker": "student",
        },
        {
            "text": "plug it __?",
            "speaker": "teacher",
        },
        {
            "text": "in",
            "speaker": "student",
        }
    ],
    "response": {
        "text": "yes, good. And when the battery is full, you need to ____ (disconnect it)",
        "speaker": "teacher",
    }
}
~~~

Each **test sample** is given as a JSON object that uses the same format as the training sample but which excludes the reference response. As a result, each test sample has two fields:

id
: a unique identifier for this sample.

utterances
: a list of utterances, which corresponds to the preceding dialogue context. Each utterance is a JSON object with a ``"text"`` field containing the utterance and a ``"speaker"`` field containing a unique label for the speaker.

~~~ json
{
    "id": "test_0000",
    "utterances": [
        {
            "text": "Yes, good! And to charge it up, you need to __ it ___",
            "speaker": "teacher",
        },
        {
            "text": "…",
            "speaker": "student",
        },
        {
            "text": "connect to the source of electricity",
            "speaker": "teacher",
        },
        {
            "text": "i understand",
            "speaker": "student",
        },
        {
            "text": "plug it __?",
            "speaker": "teacher",
        },
        {
            "text": "in",
            "speaker": "student",
        }
    ]
}
~~~ 

## Participation

The shared task is hosted on **CodaLab** {% cite codalab_competitions --file beasharedtask2023 %}. Anyone participating in the shared task will be asked to register on the CodaLab platform and to comply with the terms and conditions of using the TSCC data.

### Submission

Participants are asked to submit a **JSONL file** with the best generated teacher response for each test sample. Each line of the file corresponds to the generated result for one test sample. This result must be a JSON object with the following fields:
- an `"id"` field containing the unique identifier that was given in the test sample
- a `"text"` field containing the generated teacher utterance
- a `"speaker"` field containing a unique label to identify your system

~~~ json
{"id": "test_0000", "text": "yes!", "speaker": "My System Name"}
{"id": "test_0001", "text": "...", "speaker": "My System Name"}
{"id": "test_0002", "text": "...", "speaker": "My System Name"}
{"id": "test_0003", "text": "...", "speaker": "My System Name"}
...
~~~
    
### Evaluation

Submissions are ranked according to **automated dialogue evaluation metrics** that perform a turn-level evaluation of the generated response {% cite yeh_comprehensive_2021 --file beasharedtask2023 %}. The **leaderboard rank** is computed as the average rank on BERTScore F1 (a referenced metric for turn-level evaluation) and DialogRPT final average (a reference-free metric for turn-level evaluation).

1. We use **BERTScore** to evaluate the submitted (i.e., generated) response with respect to the reference (i.e., teacher) response. The metric matches words in submission and reference responses by cosine similarity and produces precision, recall, and F1 scores. See [https://github.com/Tiiiger/bert_score](https://github.com/Tiiiger/bert_score) for more information.
2. We use **DialogRPT** to evaluate the submitted (i.e., generated) response with respect to the preceding dialogue context. DialogRPT is a set of dialog response ranking models proposed by Microsoft Research NLP Group trained on 100+ millions of human feedback data. See [https://github.com/golsun/DialogRPT](https://github.com/golsun/DialogRPT) for more information. The following DialogRPT metrics are used:
    - **updown**: the average likelihood that the response gets the most upvotes
    - **human_vs_rand**: the average likelihood that the response is relevant for the given context
    - **human_vs_machine**: the average likelihood that the response is human-written rather than machine-generated
    - **final (avg/max)**: the (average/best) weighted ensemble score of the above metrics

The **top 3 submissions** on the automated evaluation will be targeted for further **human evaluation** on the Prolific crowdsourcing platform. During this manual evaluation, human raters compare a pair of responses (teacher - system or system - system) in terms of three abilities: speak like a teacher, understand a student, help a student {% cite tack_ai_2022  --file beasharedtask2023 %}.

### Important Dates

<span style="font-size: smaller;"><b>Note</b>: All deadlines are 11:59pm UTC-12 (anywhere on earth).</span>
{: .notice--danger}

|=====
| **Fri Mar 24, 2023** | Training data release |
| **Mon May 1, 2023** | Test data release | 
| **Fri May 5, 2023** | Final submissions due |
| **Mon May 8, 2023** | Results announced |
| **Fri May 12, 2023** | Results human evaluation announced |
| **Mon May 22, 2023** | System papers due |
| **Fri May 26, 2023** | Paper reviews returned | 
| **Tue May 30, 2023** | Camera-ready papers due | 
| **Mon June 12, 2023** | Pre-recorded video due |
| **July 13, 2023** | Workshop at ACL |
|=====
{: rules="groups"}

<!--
## Organizers

 - Anaïs Tack, KU Leuven
- Ekaterina Kochmar, University of Bath
- Zheng Yuan, King’s College London
- Serge Bibauw, Universidad Central del Ecuador
- Chris Piech, Stanford University -->

## References

{% bibliography --cited --file beasharedtask2023 %}
