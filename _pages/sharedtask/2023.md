---
layout: single
title: "BEA 2023 Shared Task"
subtitle: &subtitle "Generating AI Teacher Responses in Educational Dialogues" 
permalink: /sharedtask/2023
toc: true
sidebar:
  - title: *subtitle
    image: "/assets/images/sharedtask/2023/logo.png"
    image_alt: "shared task illustration"
    text: "
Shared task organized during the [BEA](/bea/2023) workshop at [ACL 2023](https://2023.aclweb.org).<br>
Toronto, Canada <br>
July 13, 2023"
    nav: beasharedtask2023  # <-- add navigation for CodaLab links
---

<h1>{{ page.subtitle }}</h1>

## Motivation, Goal, and Purpose

Conversational agents offer promising opportunities for education. They can fulfill various roles (e.g., intelligent tutors and service-oriented assistants) and pursue different objectives (e.g., improving student skills and increasing instructional efficiency) (Wollny et al. 2021). Among all of these different vocations of an educational chatbot, the most prevalent one is the **AI teacher** helping a student with skill improvement and providing more opportunities to practice. Some recent meta-analyses have even reported a significant effect of chatbots on skill improvement, for example in language learning (Bibauw et al. 2022). What is more, current advances in AI and natural language processing have led to the development of conversational agents that are founded on more powerful generative language models.

Despite these promising opportunities, the use of powerful generative models as a foundation for downstream tasks also presents several crucial challenges. In the educational domain in particular, it is important to ascertain whether that foundation is solid or flimsy. Bommasani et al. (2021: pp. 67-72) stressed that, if we want to put these models into practice as AI teachers, it is imperative to determine whether they can (a) speak to students like a teacher, (b) understand students, and (c) help students improve their understanding. Therefore, Tack and Piech (2022) formulated the **AI teacher test challenge**: How can we test whether state-of-the-art generative models are good AI teachers, capable of replying to a student in an educational dialogue?

Following the AI teacher test challenge, we organize a first **shared task on the generation of teacher language in educational dialogues**.
The goal of the task is to use NLP and AI methods to generate teacher responses in real-world samples of teacher-student interactions. These samples are taken from the *Teacher Student Chatroom Corpus* (Caines et al. 2020; Caines et al. 2022). Each training sample is composed of a dialogue context (i.e., several teacher-student utterances) as well as the teacher’s response. For each test sample, participants are asked to submit their best generated teacher response.

    [ DIALOGUE CONTEXT ]
    Teacher: Yes, good! And to charge it up, you need to __ it ___
    Student: …
    Teacher: connect to the source of electricity 
    Student: i understand
    Teacher: plug it __?
    Student: in

    [ REFERENCE RESPONSE ]
    Teacher: yes, good. And when the battery is full, you need to ____ (disconnect it)

The purpose of the task is to **benchmark the ability of generative models to act as AI teachers, replying to a student in a teacher-student dialogue**. Submissions are ranked according to several automated dialogue evaluation metrics, with the top submissions selected for further human evaluation. During this manual evaluation, human raters compare a pair of teacher responses in terms of three abilities: speak like a teacher, understand a student, help a student (Tack & Piech 2022). As such, we adopt an evaluation method that is akin to ACUTE-Eval for evaluating dialogue systems (Li et al. 2019).

## Data

The shared task is based on data from the *Teacher-Student Chatroom Corpus* (TSCC) (Caines et al. 2020; Caines et al. 2022). This corpus comprises data from several chatrooms (102 in total) in which an English as a second language (ESL) teacher interacts with a student in order to work on a language exercise and assess the student’s English language proficiency. 

From each dialogue, several shorter passages were extracted. Each passage is at most 100 tokens long, is composed of several sequential teacher-student turns (i.e., the preceding dialogue context), and ends with a teacher utterance (i.e., the reference response). These short passages are the data samples used in this shared task.

The format is inspired by the JSON format used in ConvoKit (Chang et al. 2020).
Each **training sample** is given as a JSON object composed of three fields:

id
: a unique identifier for this sample.

utterances
: a list of utterances, which correspond to the preceding dialogue context. Each utterance is a JSON object with a ``"text"`` field containing the utterance and a ``"speaker"`` field containing a unique label for the speaker.

response
: a reference response, which corresponds to the final teacher’s utterance. Again, this utterance is a JSON object with a ``"text"`` field containing the utterance and a ``"speaker"`` field containing a unique label for the speaker.

~~~ json
{
    "id": "train_0000",
    "utterances": [
        {
            "text": "Yes, good! And to charge it up, you need to __ it ___",
            "speaker": "teacher",
        },
        {
            "text": "…",
            "speaker": "student",
        },
        {
            "text": "connect to the source of electricity",
            "speaker": "teacher",
        },
        {
            "text": "i understand",
            "speaker": "student",
        },
        {
            "text": "plug it __?",
            "speaker": "teacher",
        },
        {
            "text": "in",
            "speaker": "student",
        }
    ],
    "response": {
        "text": "yes, good. And when the battery is full, you need to ____ (disconnect it)",
        "speaker": "teacher",
    }
}
~~~

Each **test sample** is given as a JSON object that uses the same format as the training sample but which excludes the reference response. As a result, each test sample has two fields:

id
: a unique identifier for this sample.

utterances
: a list of utterances, which corresponds to the preceding dialogue context. Each utterance is a JSON object with a ``"text"`` field containing the utterance and a ``"speaker"`` field containing a unique label for the speaker.

~~~ json
{
    "id": "test_0000",
    "utterances": [
        {
            "text": "Yes, good! And to charge it up, you need to __ it ___",
            "speaker": "teacher",
        },
        {
            "text": "…",
            "speaker": "student",
        },
        {
            "text": "connect to the source of electricity",
            "speaker": "teacher",
        },
        {
            "text": "i understand",
            "speaker": "student",
        },
        {
            "text": "plug it __?",
            "speaker": "teacher",
        },
        {
            "text": "in",
            "speaker": "student",
        }
    ]
}
~~~ 

## Participation

The shared task is hosted on [**CodaLab**](https://codalab.lisn.upsaclay.fr) (Pavao et al. 2022). Anyone participating in the shared task will be asked to:
1. Register on the CodaLab platform.
2. Fill in the [registration form](https://forms.gle/iAdKCq3dRS9srzjc6) with your CodaLab ID. In this form, you must comply with the terms and conditions of the task and the TSCC data.
3. Register for the [CodaLab competition](https://codalab.lisn.upsaclay.fr/competitions/11705) with your CodaLab ID. We will only accept people who submitted the registration form. Note that you can participate as a member of one team only.

### Submission

Participants are asked to submit a **JSONL file** with the best generated teacher response for each test sample. The CodaLab scoring program expects a **ZIP-compressed file** named `answer.jsonl`. Each line of the file corresponds to the generated result for one test sample. This result must be a JSON object with the following fields:
- an `"id"` field containing the unique identifier that was given in the test sample
- a `"text"` field containing the generated teacher utterance
- a `"speaker"` field containing a unique label to identify your system

~~~ json
{"id": "test_0000", "text": "yes!", "speaker": "My System Name"}
{"id": "test_0001", "text": "...", "speaker": "My System Name"}
{"id": "test_0002", "text": "...", "speaker": "My System Name"}
{"id": "test_0003", "text": "...", "speaker": "My System Name"}
...
~~~

The number of submissions during both the develoment and the test phases will be limited to **3 attempts** per team.

### Evaluation

Submissions are ranked according to **automated dialogue evaluation metrics** that perform a turn-level evaluation of the generated response (see Yeh et al. 2021 for a comprehensive overview).

1. We use **BERTScore** to evaluate the submitted (i.e., generated) response with respect to the reference (i.e., teacher) response. The metric matches words in submission and reference responses by cosine similarity and produces precision, recall, and F1 scores. See [https://github.com/Tiiiger/bert_score](https://github.com/Tiiiger/bert_score) for more information.
2. We use **DialogRPT** to evaluate the submitted (i.e., generated) response with respect to the preceding dialogue context. DialogRPT is a set of dialog response ranking models proposed by Microsoft Research NLP Group trained on 100+ millions of human feedback data. See [https://github.com/golsun/DialogRPT](https://github.com/golsun/DialogRPT) for more information. The following DialogRPT metrics are used:
    - **updown**: the average likelihood that the response gets the most upvotes
    - **human_vs_rand**: the average likelihood that the response is relevant for the given context
    - **human_vs_machine**: the average likelihood that the response is human-written rather than machine-generated
    - **final (avg/best)**: the (average/maximum) weighted ensemble score of all DialogRPT metrics

The **leaderboard rank** is computed as the average **rank** on BERTScore F1 (a referenced metric for turn-level evaluation) and DialogRPT final average (a reference-free metric for turn-level evaluation). If participants are tied in rank, the **tiebreaker** is the average rank on the individual scores for BERTScore (precision, recall) and DialogRPT (updown, human vs. rand, human vs. machine).

The **top 3 submissions** on the automated evaluation will be targeted for further **human evaluation** on the Prolific crowdsourcing platform. During this manual evaluation, human raters will compare a pair of responses (teacher - system or system - system) in terms of three factors: does it speak like a teacher, does it understand a student, and does it help a student (Tack & Piech 2022).

### Results

#### Phase 1: Development

<table class="display compact" style="width:100%" id="leaderboard-development">
{% assign cols = "User;BERTScore P;BERTScore R;BERTScore F1;DialogRPT updown;DialogRPT human vs rand;DialogRPT human vs machine;DialogRPT final (avg);DialogRPT final (best);Rank" | split:';' %}
<thead>
<tr>
<th colspan=2></th>
<th colspan=3>BERTScore</th>
<th colspan=5>DialogRPT</th>
<th colspan=1>Average</th>
</tr>
<tr>
<th>#</th>
<th>User</th>
<th>P</th>
<th>R</th>
<th>F1</th>
<th>updown</th>
<th>human vs rand</th>
<th>human vs machine</th>
<th>final (avg)</th>
<th>final (best)</th>
<th>Rank</th>
</tr>
</thead>
<tbody>
{% assign leaderboard = site.data.bea2023sharedtask.development_results %}
{% for row in leaderboard %}
<tr> <!-- {% if forloop.index <=3 %}style="background-color: lightsalmon;"{% endif %} -->
<td>{{ forloop.index }}</td>
{% for col in cols %}
<td>
{% if col == 'BERTScore F1' or col == 'DialogRPT final (avg)' or col == 'Rank' %}
<strong>{{ row[col] | replace: " ", "<br/>" }}</strong>
{% else %}
{{ row[col] | replace: " ", "<br/>" }}
{% endif %}
</td>
{% endfor %}
</tr>
{% endfor %}
</tbody>
</table>

#### Phase 2: Evaluation

<table class="display compact" style="width:100%" id="leaderboard-evaluation">
{% assign cols = "User;BERTScore P;BERTScore R;BERTScore F1;DialogRPT updown;DialogRPT human vs rand;DialogRPT human vs machine;DialogRPT final (avg);DialogRPT final (best);Rank" | split:';' %}
<thead>
<tr>
<th colspan=2></th>
<th colspan=3>BERTScore</th>
<th colspan=5>DialogRPT</th>
<th colspan=1>Average</th>
</tr>
<tr>
<th>#</th>
<th>User</th>
<th>P</th>
<th>R</th>
<th>F1</th>
<th>updown</th>
<th>human vs rand</th>
<th>human vs machine</th>
<th>final (avg)</th>
<th>final (best)</th>
<th>Rank</th>
</tr>
</thead>
<tbody>
{% assign leaderboard = site.data.bea2023sharedtask.evaluation_results %}
{% for row in leaderboard %}
<tr> <!-- {% if forloop.index <=3 %}style="background-color: lightsalmon;"{% endif %} -->
<td>{{ forloop.index }}</td>
{% for col in cols %}
<td>
{% if col == 'BERTScore F1' or col == 'DialogRPT final (avg)' or col == 'Rank' %}
<strong>{{ row[col] | replace: " ", "<br/>" }}</strong>
{% else %}
{{ row[col] | replace: " ", "<br/>" }}
{% endif %}
</td>
{% endfor %}
</tr>
{% endfor %}
</tbody>
</table>

<script>
$(document).ready(function () {
$('#leaderboard').DataTable({
    fixedHeader: true,
    responsive: true
});
});
</script>

### Important Dates

<span style="font-size: smaller;"><b>Note</b>: All deadlines are 11:59pm UTC-12 (anywhere on Earth).</span>
{: .notice--danger}

|=====
| **Fri Mar 24, 2023** | Training data release |
| **Mon May 1, 2023** | Test data release | 
| **Fri May 5, 2023** | Final submissions due |
| **Mon May 8, 2023** | Results announced |
| **Fri May 12, 2023** | Human evaluation results announced |
| **Mon May 22, 2023** | System papers due |
| **Fri May 26, 2023** | Paper reviews returned | 
| **Tue May 30, 2023** | Camera-ready papers due | 
| **Mon June 12, 2023** | Pre-recorded video due |
| **July 13, 2023** | Workshop at ACL |
|=====
{: rules="groups"}

### FAQ

Can participants use commercial large language models like GPT-3, ChatGPT, etc. in the competition?
: You may use third-party models as long as you are allowed to do so, correctly attribute their use, and also clearly describe how they were used. Which version/release did you use? What prompt(s) did you use? How did you set the API parameters? Did you use the default parameters or did you fine-tune the hyperparameters?

How do I go about participating in the development phase?
: First, download the **training data** (with reference responses) and the **held-out development data** (without reference responses) on CodaLab. Under the "Files" tab on the "Participate" page, click the download button for "Phase #1 Development". 
![Download the data released for the development phase](/assets/images/sharedtask/2023/how-to-dev-1-download.png)
Next, use the **training set** to develop your system.
While developing your system, you can evaluate its results on the **development set**. Generate the missing responses. Include these results in a file named "answer.jsonl" with the [expected submission format](#submission). Compress the file with ZIP. For example, you could name this ZIP-compressed file "dev_submission.zip".
Under the "Submit / View Results" tab on the "Participate" page, click on "Development". Fill in some metadata to describe your submission.
![](/assets/images/sharedtask/2023/how-to-dev-2-submission-metadata.png)
Click on "Submit" to upload the ZIP-compressed file (e.g., "dev_submission.zip"). Once the results are submitted, the scoring program will run the evaluation metrics.
![](/assets/images/sharedtask/2023/how-to-dev-3-submit-results.png)
If the submission succeeds, your results will be shown on the leaderboard.
![](/assets/images/sharedtask/2023/how-to-dev-4-results.png)

## Organizers

- [Anaïs Tack](https://anaistack.github.io/), KU Leuven
- [Ekaterina Kochmar](https://ekochmar.github.io/about/), MBZUAI
- [Zheng Yuan](https://www.cl.cam.ac.uk/~zy249/), King's College London
- [Serge Bibauw](https://serge.bibauw.be), Universidad Central del Ecuador
- [Chris Piech](https://stanford.edu/~cpiech/), Stanford University

## References

- Bibauw, Serge & Van den Noortgate, Wim & François, Thomas & Desmet, Piet. 2022. Dialogue Systems for Language Learning: A Meta-Analysis. Language Learning & Technology 26(1). accepted.
- Bommasani, Rishi & Hudson, Drew A. & Adeli, Ehsan & Altman, Russ & Arora, Simran & von Arx, Sydney & Bernstein, Michael S. et al. 2021. On the Opportunities and Risks of Foundation Models. Center for Research on Foundation Models (CRFM): Stanford University.
- Caines, Andrew & Yannakoudakis, Helen & Edmondson, Helena & Allen, Helen & Pérez-Paredes, Pascual & Byrne, Bill & Buttery, Paula. 2020. The Teacher-Student Chatroom Corpus. In , Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning, 10–20. Gothenburg, Sweden: LiU Electronic Press.
- Caines, Andrew & Yannakoudakis, Helen & Allen, Helen & Pérez-Paredes, Pascual & Byrne, Bill & Buttery, Paula. 2022. The Teacher-Student Chatroom Corpus Version 2: More Lessons, New Annotation, Automatic Detection of Sequence Shifts. In , Proceedings of the 11th Workshop on NLP for Computer Assisted Language Learning, 23–35. Louvain-la-Neuve, Belgium: LiU Electronic Press.
- Chang, Jonathan P. & Chiam, Caleb & Fu, Liye & Wang, Andrew & Zhang, Justine & Danescu-Niculescu-Mizil, Cristian. 2020. ConvoKit: A Toolkit for the Analysis of Conversations. In , Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 57–60. 1st virtual meeting: Association for Computational Linguistics.
- Li, Margaret & Weston, Jason & Roller, Stephen. 2019. ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons. arXiv. (doi:10.48550/arXiv.1909.03087)
- Macina, Jakub & Daheim, Nico & Wang, Lingzhi & Sinha, Tanmay & Kapur, Manu & Gurevych, Iryna & Sachan, Mrinmaya. 2023. Opportunities and Challenges in Neural Dialog Tutoring. arXiv (https://arxiv.org/abs/2301.09919)
- Pavao, Adrien & Guyon, Isabelle & Letournel, Anne-Catherine & Baró, Xavier & Escalante, Hugo & Escalera, Sergio & Thomas, Tyler & Xu, Zhen. 2022. CodaLab Competitions: An Open Source Platform to Organize Scientific Challenges. Technical report.
- Tack, Anaïs & Piech, Chris. 2022. The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues. In Mitrovic, Antonija & Bosch, Nigel (eds.), Proceedings of the 15th International Conference on Educational Data Mining, vol. 15, 522–529. Durham, United Kingdom: International Educational Data Mining Society. (doi:10.5281/zenodo.6853187)
- Wollny, Sebastian & Schneider, Jan & Di Mitri, Daniele & Weidlich, Joshua & Rittberger, Marc & Drachsler, Hendrik. 2021. Are We There Yet? - A Systematic Literature Review on Chatbots in Education. Frontiers in Artificial Intelligence 4. 654924. (doi:10.3389/frai.2021.654924)
- Yeh, Yi-Ting & Eskenazi, Maxine & Mehri, Shikib. 2021. A Comprehensive Assessment of Dialog Evaluation Metrics. In , The First Workshop on Evaluations and Assessments of Neural Conversation Systems, 15–33. Online: Association for Computational Linguistics. (doi:10.18653/v1/2021.eancs-1.3)
