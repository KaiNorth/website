@inproceedings{tack_ai_2022,
  title = {The {{AI Teacher Test}}: {{Measuring}} the {{Pedagogical Ability}} of {{Blender}} and {{GPT-3}} in {{Educational Dialogues}}},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Educational Data Mining}}},
  author = {Tack, Ana{\"i}s and Piech, Chris},
  editor = {Mitrovic, Antonija and Bosch, Nigel},
  year = {2022},
  month = jul,
  volume = {15},
  pages = {522--529},
  publisher = {{International Educational Data Mining Society}},
  address = {{Durham, United Kingdom}},
  doi = {10.5281/zenodo.6853187},
  abstract = {How can we test whether state-of-the-art generative models, such as Blender and GPT-3, are good AI teachers, capable of replying to a student in an educational dialogue? Designing an AI teacher test is challenging: although evaluation methods are much-needed, there is no off-the-shelf solution to measuring pedagogical ability. This paper reports on a first attempt at an AI teacher test. We built a solution around the insight that you can run conversational agents in parallel to human teachers in real-world dialogues, simulate how different agents would respond to a student, and compare these counterpart responses in terms of three abilities: speak like a teacher, understand a student, help a student. Our method builds on the reliability of comparative judgments in education and uses a probabilistic model and Bayesian sampling to infer estimates of pedagogical ability. We find that, even though conversational agents (Blender in particular) perform well on conversational uptake, they are quantifiably worse than real teachers on several pedagogical dimensions, especially with regard to helpfulness (Blender: {$\Delta$} ability = -0.75; GPT-3: {$\Delta$} ability = -0.93).},
  copyright = {All rights reserved},
  isbn = {978-1-73367-363-1},
  annotation = {2022.EDM-short-papers.54}
}

@article{wollny_are_2021,
  title = {Are {{We There Yet}}? - {{A Systematic Literature Review}} on {{Chatbots}} in {{Education}}},
  shorttitle = {Are {{We There Yet}}?},
  author = {Wollny, Sebastian and Schneider, Jan and Di Mitri, Daniele and Weidlich, Joshua and Rittberger, Marc and Drachsler, Hendrik},
  year = {2021},
  month = jul,
  journal = {Frontiers in Artificial Intelligence},
  volume = {4},
  pages = {654924},
  issn = {2624-8212},
  doi = {10.3389/frai.2021.654924},
  abstract = {Chatbots are a promising technology with the potential to enhance workplaces and everyday life. In terms of scalability and accessibility, they also offer unique possibilities as communication and information tools for digital learning. In this paper, we present a systematic literature review investigating the areas of education where chatbots have already been applied, explore the pedagogical roles of chatbots, the use of chatbots for mentoring purposes, and their potential to personalize education. We conducted a preliminary analysis of 2,678 publications to perform this literature review, which allowed us to identify 74 relevant publications for chatbots' application in education. Through this, we address five research questions that, together, allow us to explore the current state-of-the-art of this educational technology. We conclude our systematic review by pointing to three main research challenges: 1) Aligning chatbot evaluations with implementation objectives, 2) Exploring the potential of chatbots for mentoring students, and 3) Exploring and leveraging adaptation capabilities of chatbots. For all three challenges, we discuss opportunities for future research.},
  file = {/Users/anais/iCloud Drive/Zotero/Journal Article/W/WollnyS et al/2021/wollny_et_al_2021_are_we_there_yet.pdf}
}

@article{bibauw_dialogue_2022,
  title = {Dialogue Systems for Language Learning: A Meta-Analysis},
  author = {Bibauw, Serge and {Van den Noortgate}, Wim and Fran{\c c}ois, Thomas and Desmet, Piet},
  year = {2022},
  journal = {Language Learning \& Technology},
  volume = {26},
  number = {1},
  pages = {accepted},
  file = {/Users/anais/iCloud Drive/Zotero/Journal Article/B/BibauwS et al/2022/bibauw_et_al_2022_dialogue_systems_for_language_learning.pdf}
}

@techreport{bommasani_opportunities_2021,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Kohd, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2021},
  month = aug,
  eprint = {2108.07258},
  eprinttype = {arxiv},
  address = {{Center for Research on Foundation Models (CRFM)}},
  institution = {{Stanford University}},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/anais/iCloud Drive/Zotero/Report/B/BommasaniR et al/2021/bommasani_et_al_2021_on_the_opportunities_and_risks_of_foundation_models.pdf}
}

@inproceedings{caines_teacherstudent_2020,
  title = {The Teacher-Student Chatroom Corpus},
  booktitle = {Proceedings of the 9th Workshop on {{NLP}} for Computer Assisted Language Learning},
  author = {Caines, Andrew and Yannakoudakis, Helen and Edmondson, Helena and Allen, Helen and {P{\'e}rez-Paredes}, Pascual and Byrne, Bill and Buttery, Paula},
  year = {2020},
  month = nov,
  pages = {10--20},
  publisher = {{LiU Electronic Press}},
  address = {{Gothenburg, Sweden}},
  file = {/Users/anais/iCloud Drive/Zotero/Conference Paper/C/CainesA et al/2020/caines_et_al_2020_the_teacher-student_chatroom_corpus.pdf}
}

@misc{li_acuteeval_2019,
  title = {{{ACUTE-EVAL}}: {{Improved Dialogue Evaluation}} with {{Optimized Questions}} and {{Multi-turn Comparisons}}},
  shorttitle = {{{ACUTE-EVAL}}},
  author = {Li, Margaret and Weston, Jason and Roller, Stephen},
  year = {2019},
  month = sep,
  number = {arXiv:1909.03087},
  eprint = {1909.03087},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1909.03087},
  abstract = {While dialogue remains an important end-goal of natural language research, the difficulty of evaluation is an oft-quoted reason why it remains troublesome to make real progress towards its solution. Evaluation difficulties are actually two-fold: not only do automatic metrics not correlate well with human judgments, but also human judgments themselves are in fact difficult to measure. The two most used human judgment tests, single-turn pairwise evaluation and multi-turn Likert scores, both have serious flaws as we discuss in this work. We instead provide a novel procedure involving comparing two full dialogues, where a human judge is asked to pay attention to only one speaker within each, and make a pairwise judgment. The questions themselves are optimized to maximize the robustness of judgments across different annotators, resulting in better tests. We also show how these tests work in self-play model chat setups, resulting in faster, cheaper tests. We hope these tests become the de facto standard, and will release open-source code to that end.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/anais/iCloud Drive/Zotero/Report/L/LiM et al/2019/li_et_al_2019_acute-eval.pdf;/Users/anais/Zotero/storage/XAJVD5YD/1909.html}
}

@inproceedings{yeh_comprehensive_2021,
  title = {A {{Comprehensive Assessment}} of {{Dialog Evaluation Metrics}}},
  booktitle = {The {{First Workshop}} on {{Evaluations}} and {{Assessments}} of {{Neural Conversation Systems}}},
  author = {Yeh, Yi-Ting and Eskenazi, Maxine and Mehri, Shikib},
  year = {2021},
  month = nov,
  pages = {15--33},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eancs-1.3},
  abstract = {Automatic evaluation metrics are a crucial component of dialog systems research. Standard language evaluation metrics are known to be ineffective for evaluating dialog. As such, recent research has proposed a number of novel, dialog-specific metrics that correlate better with human judgements. Due to the fast pace of research, many of these metrics have been assessed on different datasets and there has as yet been no time for a systematic comparison between them. To this end, this paper provides a comprehensive assessment of recently proposed dialog evaluation metrics on a number of datasets. In this paper, 23 different automatic evaluation metrics are evaluated on 10 different datasets. Furthermore, the metrics are assessed in different settings, to better qualify their respective strengths and weaknesses. This comprehensive assessment offers several takeaways pertaining to dialog evaluation metrics in general. It also suggests how to best assess evaluation metrics and indicates promising directions for future work.},
  file = {/Users/anais/iCloud Drive/Zotero/Conference Paper/Y/YehY et al/2021/yeh_et_al_2021_a_comprehensive_assessment_of_dialog_evaluation_metrics.pdf}
}

@inproceedings{chang_convokit_2020,
  title = {{{ConvoKit}}: {{A Toolkit}} for the {{Analysis}} of {{Conversations}}},
  shorttitle = {{{ConvoKit}}},
  booktitle = {Proceedings of the 21th {{Annual Meeting}} of the {{Special Interest Group}} on {{Discourse}} and {{Dialogue}}},
  author = {Chang, Jonathan P. and Chiam, Caleb and Fu, Liye and Wang, Andrew and Zhang, Justine and {Danescu-Niculescu-Mizil}, Cristian},
  year = {2020},
  month = jul,
  pages = {57--60},
  publisher = {{Association for Computational Linguistics}},
  address = {{1st virtual meeting}},
  abstract = {This paper describes the design and functionality of ConvoKit, an open-source toolkit for analyzing conversations and the social interactions embedded within. ConvoKit provides an unified framework for representing and manipulating conversational data, as well as a large and diverse collection of conversational datasets. By providing an intuitive interface for exploring and interacting with conversational data, this toolkit lowers the technical barriers for the broad adoption of computational methods for conversational analysis.},
  file = {/Users/anais/iCloud Drive/Zotero/Conference Paper/C/ChangJ et al/2020/chang_et_al_2020_convokit.pdf}
}
