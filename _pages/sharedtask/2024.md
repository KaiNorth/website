---
layout: single
title: "BEA 2024 Shared Task"
subtitle: &subtitle "Automated Prediction of Item Difficulty and Item Response Time" 
permalink: /sharedtask/2024
toc: true
sidebar:
  - title: *subtitle
    image: ""
    image_alt: ""
    text: "
Shared task organized during the [BEA](/bea/2024) workshop at [NAACL 2024](https://sig-edu.org/bea/2024).<br>
Mexico City, Mexico <br>
June 20 or 21, 2024"
    nav: beasharedtask2024 # <-- add navigation for CodaLab links.
---

## Motivation

For standardized exams to be fair and valid, test items must meet certain criteria. One important criterion is that the questions should cover a wide range of difficulty levels to gather information about the abilities of test takers effectively. Additionally, it is essential to allocate an appropriate amount of time for each question: too little time can make the exam speeded, while too much time can make it inefficient. Typically, item difficulty and response time data are collected via a process called pretesting, where new items are embedded in live exams alongside scored items. While robust, this process of collecting item characteristics data is time-consuming and expensive. As noted by Settles et al. (2020), "This labor-intensive process often restricts the number of items that can feasibly be created, which in turn poses a threat to security: Items may be copied and leaked, or simply used too often".

To address this challenge (also referred to as the "cold-start parameter estimation problem" (McCarthy et al., 2021)), there is growing interest in predicting item characteristics such as difficulty and response time based on the item text. Such estimates can be used to "jump-start" parameter estimation by exposing the item to a smaller sample of test-takers, or improve fairness by reducing the time variance for forms that include pretest items (Baldwin et al., 2020). 

Due to difficulties with sharing exam data, efforts to advance the state-of-the-art in item parameter prediction have been fragmented and conducted in individual institutions, with no transparent evaluation on a publicly available dataset. In this Shared Task, we bridge this gap by sharing practice item content and characteristics from a high-stakes medical exam called the United States Medical Licensing Examination® (USMLE®) for the exploration of two topics: predicting item difficulty (Track 1) and item response time (Track 2) based on item text.


## Data

The data for the Shared Task consists of 675 previously used and now retired Multiple Choice Questions (MCQs) from USMLE Steps 1, 2 CK, and 3. The USMLE is a series of examinations (called Steps) to support medical licensure decisions in the United States that is developed by the National Board of Medical Examiners (NBME) and Federation of State Medical Boards (FSMB). An example practice item from the dataset is given in Table 1. 

| Q   | A 65-year-old woman comes to the physician for a follow-up examination after blood pressure measurements were 175/105 mm Hg and 185/110 mm Hg 1 and 3 weeks ago, respectively. She has well-controlled type 2 diabetes mellitus. Her blood pressure now is 175/110 mm Hg. Physical examination shows no other abnormalities. Antihypertensive therapy is started, but her blood pressure remains elevated at her next visit 3 weeks later. Laboratory studies show increased plasma renin activity; the erythrocyte sedimentation rate and serum electrolytes are within the reference ranges. Angiography shows a high-grade stenosis of the proximal right renal artery; the left renal artery appears normal. Which of the following is the most likely diagnosis?  |
|-----|:--|
| (A) | Atherosclerosis  |
| (B) | Congenital renal artery hypoplasia |
| (C) | Fibromuscular dysplasia  |
| (D) | Takayasu arteritis  |
| (E) | Temporal arteritis  |

*Table 1: An example of a practice item from the USMLE Step 1 Sample Test Questions at usmle.org*

The part describing the case is referred to as stem, the correct answer is referred to as key, and the incorrect answer options are known as distractors. All items are MCQs that test medical knowledge and were written by experienced subject matter experts following a set of guidelines, stipulating adherence to a standard structure. These guidelines require avoidance of “window dressing” (extraneous material not needed to answer the item), “red herrings” (information designed to mislead the test-taker), and grammatical cues (e.g., correct answers that are longer or more specific than the other options). The goal of standardizing items in this manner is to produce items that vary in their difficulty and discriminating power due only to differences in the medical content they assess. 

The items were administered within a standard nine-hour exam. For this shared task, the item characteristic data was derived from first-time examinees from accredited US and Canadian medical schools.

Each item is tagged with the following item characteristics:

- **Item difficulty**: A measure of item difficulty where higher values indicate a higher proportion of examinees answered the item correctly (less difficult items).

- **Time intensity**: arithmetic mean response time, measured in seconds, across all examinees who attempted a given item in a live exam. This includes all time spent on the item from the moment it is presented on the screen until the examinee moves to the next item, as well as any revisits.


The data is structured as follows:

| ItemNum | ItemStem_Text | Answer__A | Answer__B| Answer__C| Answer__D| Answer__E| (…) | Answer_Text | PVALUE | Response_Time | EXAM |
|---|---|---|---|---|---|---|---|---|---|---|---|

***ItemNum*** denotes the consecutive number of the item in the dataset (e.g., 1,2,3,4,5, etc).

***ItemStem_Text*** contains the text data for the item stem (the part of the item describing the clinical case).

***Answer__A*** contains the text for response option A

***Answer__B*** contains the text for response option B

***Answer__C*** contains the text for response option C.
  
***(…)***

***Answer__K*** contains the text for response option K. For items that have fewer than K response options, the remaining columns are left blank. For example, if an item contains response options A to E, the fields for columns F to K are left blank for that item.

***Answer_Text*** contains the text of the correct response for the item.

***Difficulty*** contains the item difficulty measure for the item.

***Response_Time*** contains the mean response time for the item measured in seconds.

***EXAM*** denotes the Step of the USMLE exam the item belongs to (Step 1, Step 2, or Step 3). For more information on the Steps of the USMLE see https://www.usmle.org/step-exams. 

Prior work related to modeling item difficulty and time intensity for clinical MCQs from the USMLE includes the following articles: Ha et al. (2019), Baldwin et al. (2020), Yaneva et al. (2020), Xue et al. (2020), Yaneva et al. (2021) (see the References section below).


## Participation

We frame the proposed task in two separate tracks as follows:

- **Track 1**: Given the item text and metadata, predict the item difficulty variable.

- **Track 2**: Given the item text and metadata, predict the time intensity variable.

Use of one target variable in the prediction of another is not allowed, since at the time of writing of each item neither the difficulty, nor the time intensity parameters are available. 

Training data outside of the specified training set is allowed, provided that it is publicly available.

In both tracks, the evaluation will be based on the Root Mean Squared Error metric (RMSE).


## Data Access, Submission, and Evaluation

Information on the procedure for team registration, how to access the data, and how to prepare the submission files is TBA. Follow this space for updates.


## Important Dates

Training data release: January 15

Test data release: February 10

Results due: February 16

Announcement of winners: February 21

Paper submissions due: March 10

Camera-ready papers due: April 22


## Organizers

Victoria Yaneva, National Board of Medical Examiners

Peter Baldwin, National Board of Medical Examiners

Kai North, George Mason University

Brian Clauser, National Board of Medical Examiners

Saed Rezayi, National Board of Medical Examiners

Yiyun Zhou, National Board of Medical Examiners

Polina Harik, National Board of Medical Examiners


## References

Baldwin, P., Yaneva, V., Mee, J., Clauser, B. E. , Ha, L. A. 2020. Using Natural Language Processing to Predict Item Response Times and Improve Test Construction. Journal of Educational Measurement, Wiley. DOI: https://doi.org/10.1111/jedm.12264

McCarthy, A.D., Yancey, K.P., LaFlair, G.T., Egbert, J., Liao, M. and Settles, B., 2021, November. Jump-starting item parameters for adaptive language tests. In Proceedings of the 2021 conference on empirical methods in natural language processing (pp. 883-899).

Ha, L. A., Yaneva, V., Baldwin, P. and Mee, J. 2019. Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam. Proceedings of the 14th Workshop on Innovative Use of NLP for Building Educational Applications (BEA), held in conjunction with ACL 2019, Florence, Italy, 2 August, 2019.

Settles, B., T. LaFlair, G. and Hagiwara, M., 2020. Machine learning–driven language assessment. Transactions of the Association for computational Linguistics, 8, pp.247-263.

Xue, K., Yaneva, V., Runyon, C. and Baldwin, P., 2020, July. Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications (pp. 193-197).

Yaneva, V., Ha, L. A., Baldwin, P. and Mee, J. 2020. Predicting Item Survival for Multiple Choice Questions in a High-stakes Medical Exam. In Proceedings of The 12th Language Resources and Evaluation Conference (pp. 6812-6818).

Yaneva, V., Jurich, D. P., Ha, L. A., and Baldwin, P. (2021) Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical MCQs. Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications (pp. 223-232)

